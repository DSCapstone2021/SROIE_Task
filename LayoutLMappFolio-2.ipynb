{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LayoutLMappFolio.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_94XGeLz0dK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bc3bc5a-db8e-40bf-85b3-aab6e884f403"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 34.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 34.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=8d7f3407fea3f7bdd627934d0c9a9c217d2542f9cf605a8fc149b8860fe36357\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj4z-chfegmu"
      },
      "source": [
        "# Using Cloud Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANiRVVzyfDW2"
      },
      "source": [
        "Authenticate with Google Account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzRoBb3Td4Z7"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1MSPwzifIQw"
      },
      "source": [
        "Install GCloud SDK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPXkuAIUd7yI"
      },
      "source": [
        "!curl https://sdk.cloud.google.com | bash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfgKdgKRfOGu"
      },
      "source": [
        "Initialize SDK - need to answer some questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JG_iCasd8mL"
      },
      "source": [
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyHglGwWfV6_"
      },
      "source": [
        "Download files\n",
        "\n",
        "* !gsutil cp URI .  You can replace the . with the name you want\n",
        "* To get URI go to file you want to dowload in Google Cloud Platform and copy it "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1X9jNMzfYg8",
        "outputId": "36459174-67f9-4b61-8085-919ab5762151"
      },
      "source": [
        "!gsutil cp gs://capestone-benchmark-data/apm/metadata/0002cf94-5d45-48ef-9ca9-14c7e85ecef1.json metadata.json\n",
        "!gsutil cp gs://capestone-benchmark-data/apm/box/0002cf94-5d45-48ef-9ca9-14c7e85ecef1.csv annotation.csv\n",
        "!gsutil cp gs://capestone-benchmark-data/apm/img/0002cf94-5d45-48ef-9ca9-14c7e85ecef1.png image.png"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://capestone-benchmark-data/apm/metadata/0002cf94-5d45-48ef-9ca9-14c7e85ecef1.json...\n",
            "/ [0 files][    0.0 B/  227.0 B]                                                \r/ [1 files][  227.0 B/  227.0 B]                                                \r\n",
            "Operation completed over 1 objects/227.0 B.                                      \n",
            "Copying gs://capestone-benchmark-data/apm/box/0002cf94-5d45-48ef-9ca9-14c7e85ecef1.csv...\n",
            "/ [1 files][ 18.2 KiB/ 18.2 KiB]                                                \n",
            "Operation completed over 1 objects/18.2 KiB.                                     \n",
            "Copying gs://capestone-benchmark-data/apm/img/0002cf94-5d45-48ef-9ca9-14c7e85ecef1.png...\n",
            "/ [1 files][  4.6 MiB/  4.6 MiB]                                                \n",
            "Operation completed over 1 objects/4.6 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkClDjPvaTiO"
      },
      "source": [
        "# Model Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzRfdrQjz4wT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7475f352-eae3-46ee-9a36-509d9738d623"
      },
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "labels = [\"Total\", \"Other\"]\n",
        "label_map = {i: label for i, label in enumerate(labels)}\n",
        "type(label_map)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkDdxaXk0SUp"
      },
      "source": [
        "from transformers import LayoutLMModel, LayoutLMConfig, LayoutLMTokenizer\n",
        "\n",
        "# Define Tokenizer\n",
        "tokenizer = LayoutLMTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXWE6Kr72DLw"
      },
      "source": [
        "#ari's code_box for dataloader\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "# Define pytorch Dataset\n",
        "\n",
        "class InvoiceDataset(Dataset):\n",
        "\n",
        "    def __init__(self, csv_file, metadata):\n",
        "        self.annotations = pd.read_csv(csv_file, index_col=0).to_numpy()\n",
        "        self.metadata = metadata\n",
        "        self.sequence_length = 32\n",
        "        self.pre_process(self.annotations, self.metadata)\n",
        "\n",
        "    def pre_process(self, annotations, metadata):\n",
        "        for index, row in enumerate(self.annotations):\n",
        "            if row[6] == metadata[\"AMOUNT\"]:\n",
        "                self.annotations[index][7] = \"total\"\n",
        "            else:\n",
        "                self.annotations[index][7] = \"other\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        x = self.annotations[idx]\n",
        "        temp = tokenizer(x[6])\n",
        "        input_id = temp['input_ids']\n",
        "        input_id.extend([0] * (self.sequence_length - len(temp['input_ids']))) \n",
        "        input_id = torch.tensor([input_id], dtype=torch.long)\n",
        "        label = torch.tensor([[1] * self.sequence_length]) if x[7] == \"total\" else torch.tensor([[-1] * self.sequence_length])\n",
        "\n",
        "        return input_id, label\n",
        "    def normalize_bbox(bbox, width, height):\n",
        "        # Width and height correspond to those of the original image\n",
        "        return [\n",
        "            int(1000 * (bbox[0] / width)),\n",
        "            int(1000 * (bbox[1] / height)),\n",
        "            int(1000 * (bbox[2] / width)),\n",
        "            int(1000 * (bbox[3] / height)),\n",
        "        ]\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si0ko8xcfFqJ"
      },
      "source": [
        "#copy of dataloader code box - working on it to create InvoiceDataset\r\n",
        "import torch \r\n",
        "import torchvision\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "\r\n",
        "# Define pytorch Dataset\r\n",
        "\r\n",
        "class InvoiceDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self, root_dir, transform = None):\r\n",
        "        self.annotations = __get_data__(root_dir + '/box')\r\n",
        "        self.root_dir = root_dir # Directory with all the images\r\n",
        "\r\n",
        "    def __get_data_one_file__(path):\r\n",
        "      #using path to annotations\r\n",
        "      invoice = pd.read_csv(path,header=0)\r\n",
        "      invoice = invoice[['x0','x2','y0','y2','page_width','page_height','content']]\r\n",
        "      invoice_df = []\r\n",
        "      for i in invoice:\r\n",
        "        invoice_df.extend([normalize_bbox([i['x0'],i['x2'],i['y0'],i['y2']], i['page_width'], i['page_height']), i['content']])\r\n",
        "      return invoice_df\r\n",
        "\r\n",
        "    def __get_data__(root_dir_annotations):\r\n",
        "      annotation_files = sorted(glob(root_dir_annotations+'/*.csv'))\r\n",
        "\r\n",
        "      all_invoices_annotations = []\r\n",
        "\r\n",
        "      for i in annotation_files:\r\n",
        "        invoice_df = get_data_one_file(i)\r\n",
        "        all_invoices_annotations = pd.concat([all_invoices, invoice_df], axis=0, ignore_index = True)\r\n",
        "        \r\n",
        "      return(all_invoices_annotations)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return (self.annotations).shape[0]\r\n",
        "    \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return self.annotations[i]\r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "def normalize_bbox(bbox, width, height):\r\n",
        "     # Width and height correspond to those of the original image\r\n",
        "     return [\r\n",
        "         int(1000 * (bbox[0] / width)),\r\n",
        "         int(1000 * (bbox[1] / height)),\r\n",
        "         int(1000 * (bbox[2] / width)),\r\n",
        "         int(1000 * (bbox[3] / height)),\r\n",
        "     ]\r\n",
        "\r\n",
        "# # idx = 0\r\n",
        "# # bbox = normalize_bbox([df.loc[idx,'x0'],df.loc[idx,'x2'],df.loc[idx,'y0'],df.loc[idx,'y2']], df.loc[idx,'page_width'], df.loc[idx,'page_height'])\r\n",
        "\r\n",
        "# path_to_datasets = ...\r\n",
        "# data_sets = datasets.DatasetFolder(path_to_datasets, loader=get_data_one_file, extensions=['.csv'])\r\n",
        "# train_loader = torch.utils.data.DataLoader(data_sets, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbsj7LKu0djV"
      },
      "source": [
        "from transformers import LayoutLMForTokenClassification\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define model\n",
        "model = LayoutLMForTokenClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\", num_labels=len(labels))\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC8EYmr3PbIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca86b07-a53a-4b17-eece-9b836f2a1d9e"
      },
      "source": [
        "# Try forward pass with a single example\n",
        "sequence_length = 500\n",
        "bbox = [160, 301, 58, 113]\n",
        "content = \"TXU\"\n",
        "label = \"Other\"\n",
        "\n",
        "# bbox_pad = torch.zeros(4)\n",
        "# bbox = normalize_bbox([160,301,58,13], 2000, 2600)\n",
        "bbox=[[[1,2,3,5], [1,2,45,2], [6,5,42,1], [7,52,12,9]]]\n",
        "\n",
        "bbox = torch.tensor(bbox, dtype=torch.long)\n",
        "# bbox_pad[:4] = bbox\n",
        "print(\"bbox: \",bbox)\n",
        "temp = tokenizer(content)\n",
        "print(tokenizer.decode(temp['input_ids']))\n",
        "print(\"type input_ids = \", type(temp['input_ids']))\n",
        "input_ids = torch.tensor([temp['input_ids']], dtype=torch.long)\n",
        "print('input_ids: ', input_ids)\n",
        "token_type_ids = torch.tensor([temp['token_type_ids']], dtype=torch.long)\n",
        "labels = torch.tensor([[1, 0, 0, 0]])\n",
        "print(labels.shape)\n",
        "attention_mask = torch.tensor([temp['attention_mask']], dtype=torch.long)\n",
        "print(input_ids, token_type_ids, labels, attention_mask)\n",
        "\n",
        "print(bbox.shape, input_ids.shape, attention_mask.shape, token_type_ids.shape, labels.shape)\n",
        "# out = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
        "\n",
        "out = model(input_ids = input_ids, labels = labels)\n",
        "print(out)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bbox:  tensor([[[ 1,  2,  3,  5],\n",
            "         [ 1,  2, 45,  2],\n",
            "         [ 6,  5, 42,  1],\n",
            "         [ 7, 52, 12,  9]]])\n",
            "[CLS] txu [SEP]\n",
            "type input_ids =  <class 'list'>\n",
            "input_ids:  tensor([[  101, 19067,  2226,   102]])\n",
            "torch.Size([1, 4])\n",
            "tensor([[  101, 19067,  2226,   102]]) tensor([[0, 0, 0, 0]]) tensor([[1, 0, 0, 0]]) tensor([[1, 1, 1, 1]])\n",
            "torch.Size([1, 4, 4]) torch.Size([1, 4]) torch.Size([1, 4]) torch.Size([1, 4]) torch.Size([1, 4])\n",
            "TokenClassifierOutput(loss=tensor(0.5642, grad_fn=<NllLossBackward>), logits=tensor([[[-0.4792,  0.2586],\n",
            "         [ 0.1359,  0.1439],\n",
            "         [ 0.0927,  0.0447],\n",
            "         [-0.4061, -0.8401]]], grad_fn=<AddBackward0>), hidden_states=None, attentions=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx76fljeWyLa"
      },
      "source": [
        "# Try forward pass with a example of 32 padding\r\n",
        "sequence_length = 32\r\n",
        "bbox = [160, 301, 58, 113]\r\n",
        "content = \"TXU\"\r\n",
        "label = \"Other\"\r\n",
        "width = 2000\r\n",
        "height = 2600\r\n",
        "\r\n",
        "bbox = normalize_bbox(bbox, width, height)\r\n",
        "bbox = torch.tensor(bbox, dtype=torch.long)\r\n",
        "print(bbox)\r\n",
        "\r\n",
        "temp = tokenizer(content)\r\n",
        "print(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI-ZxThVcpOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078fd9a8-bd5f-4e0c-c2c0-fa773bc950fc"
      },
      "source": [
        "\n",
        "words = [\"worthless\", \"world\"]\n",
        "normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n",
        "token_boxes = []\n",
        "for word, box in zip(words, normalized_word_boxes): \n",
        "    word_tokens = tokenizer.tokenize(word)\n",
        "    print(len(word_tokens))\n",
        "    token_boxes.extend([box] * len(word_tokens))\n",
        "# add bounding boxes of cls + sep tokens\n",
        "token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n",
        "encoding = tokenizer(' '.join(words), return_tensors=\"pt\")\n",
        "input_ids = encoding[\"input_ids\"]\n",
        "attention_mask = encoding[\"attention_mask\"]\n",
        "token_type_ids = encoding[\"token_type_ids\"]\n",
        "bbox = torch.tensor([token_boxes])\n",
        "outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "# last_hidden_states = outputs.last_hidden_state\n",
        "print(bbox, input_ids.shape, attention_mask, labels.shape)\n",
        "print(word_tokens)\n",
        "print(outputs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "1\n",
            "tensor([[[   0,    0,    0,    0],\n",
            "         [ 637,  773,  693,  782],\n",
            "         [ 698,  773,  733,  782],\n",
            "         [1000, 1000, 1000, 1000]]]) torch.Size([1, 4]) tensor([[1, 1, 1, 1]]) torch.Size([1, 4])\n",
            "['world']\n",
            "TokenClassifierOutput(loss=None, logits=tensor([[[-0.1785, -0.1037],\n",
            "         [-0.1754,  0.0262],\n",
            "         [-0.0732, -0.1393],\n",
            "         [-0.1804, -0.1038]]], grad_fn=<AddBackward0>), hidden_states=None, attentions=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axd03XrswUWH"
      },
      "source": [
        "# Initialize Dataset\n",
        "invoice_dataset = InvoiceDataset(\"annotation.csv\", {\"AMOUNT\":52.62})\n",
        "train_dataloader = DataLoader(invoice_dataset,\n",
        "                              batch_size=1)\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    print(batch[0].squeeze(0).shape, batch[1].squeeze(0).shape, batch[0].squeeze(0))\n",
        "    model(input_ids = batch[0].squeeze(0), labels = batch[1].squeeze(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHHCtTmE1iSL"
      },
      "source": [
        "# Training\n",
        "\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "global_step = 0\n",
        "num_train_epochs = 5\n",
        "t_total = len(train_dataloader) * num_train_epochs # total number of training steps \n",
        "\n",
        "#put the model in training mode\n",
        "model.train()\n",
        "for epoch in range(num_train_epochs):\n",
        "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
        "        input_ids = batch[0].to(device)\n",
        "    #   bbox = batch[4].to(device)\n",
        "    #   attention_mask = batch[1].to(device)\n",
        "    #   token_type_ids = batch[2].to(device)\n",
        "    #   labels = batch[3].to(device)\n",
        "        labels = batch[1].to(device)\n",
        "\n",
        "        # forward pass\n",
        "    #   outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
        "    #                   labels=labels)\n",
        "        # print(\"\\ninput_ids: \", input_ids.squeeze(0).shape)\n",
        "        # print(\"labels: \", labels.squeeze(0).shape)\n",
        "        outputs = model(input_ids = input_ids.squeeze(0), labels = labels.squeeze(0))\n",
        "        loss = outputs.loss\n",
        "        print(\"here!\")\n",
        "        # print loss every 100 steps\n",
        "        if global_step % 100 == 0:\n",
        "            print(f\"Loss after {global_step} steps: {loss.item()}\")\n",
        "\n",
        "        # backward pass to get the gradients \n",
        "        loss.backward()\n",
        "\n",
        "        #print(\"Gradients on classification head:\")\n",
        "        #print(model.classifier.weight.grad[6,:].sum())\n",
        "\n",
        "        # update\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        global_step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVTS0zYw5jEg"
      },
      "source": [
        "# Legacy Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh86s-_M5k_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f20325-d3a2-4504-9dab-a52c4b346478"
      },
      "source": [
        "coordinates = [50, 80, 70, 100]\n",
        "sequence = \"total\"\n",
        "metadata = {\"UID\":\"0002cf94-5d45-48ef-9ca9-14c7e85ecef1\",\"AMOUNT\":52.62,\"WO_NUM\":None,\"IMI_REFERENCE\":\"100072514116\",\"INVOICE_CHECK_MEMO\":\"100072514116\",\"TXN_REFERENCE\":None,\"VHOST\":\"butlerpropertycompany\",\"CREATED_AT\":\"2020-11-06 21:48\"}\n",
        "metadata['AMOUNT']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52.62"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eXDCUcHeRnp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89pgzbSTaZzv"
      },
      "source": [
        "from torch.nn.functional import pad\r\n",
        "\r\n",
        "labels = torch.tensor([[1, 0, 0, 0]])\r\n",
        "labels\r\n",
        "# m = ConstantPad1d(32,0)\r\n",
        "# m(labels)\r\n",
        "\r\n",
        "source_pad = pad(labels, pad=(0, 0, 0, 70 - labels.shape[0]))\r\n",
        "source_pad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeNLnDDYbMet"
      },
      "source": [
        "with open()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSEmxpH8flms"
      },
      "source": [
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "with open(\"metadata.json\") as json_file:\n",
        "    data = json.load(json_file)\n",
        "with open(\"annotation.csv\") as csv_file:\n",
        "    rows = {}\n",
        "    rows['form'] = []\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_reader:\n",
        "        continue\n",
        "        # print(row)\n",
        "df = pd.read_csv(\"annotation.csv\", index_col=0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAXxWVs1hP2_",
        "outputId": "0828374e-0eb2-4747-f02c-e2b88b4abf82"
      },
      "source": [
        "df_arr = df.to_numpy()\n",
        "df_arr\n",
        "for index, row in enumerate(df_arr):\n",
        "    if row[6] == data[\"AMOUNT\"]:\n",
        "        df_arr[index][7] = \"total\"\n",
        "    else:\n",
        "        df_arr[index][7] = \"other\"\n",
        "# df_arr[5]\n",
        "x = df_arr[1]\n",
        "sequence_length = 32\n",
        "temp = tokenizer(x[6])\n",
        "input_id = temp['input_ids']\n",
        "input_id.extend([0] * (32 - len(temp['input_ids'])))\n",
        "print(input_id)\n",
        "input_id = torch.tensor([input_id], dtype=torch.long)\n",
        "print(input_id.shape)\n",
        "# asd = torch.tensor([[1] * sequence_length])\n",
        "# print(input_id.shape, asd.shape)\n",
        "# out = model(input_ids = input_id, labels = asd)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 2943, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "torch.Size([1, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHiXwtzKkdj6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}